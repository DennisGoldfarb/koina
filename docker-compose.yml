services:
  serving:
    image: nvcr.io/nvidia/tritonserver:22.03-py3
    command: #nvidia-smi
        tritonserver
        --model-store=/models
        --allow-grpc=true
        --grpc-port=8502
        --allow-http=true
        --http-port=8503
        --log-info=true
        --log-warning=true
        --log-error=true
    ports:
      - 8502:8502
      - 8503:8503
    volumes:
      - ./models:/models
    shm_size: 1GB
